{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on tutorial: Traffic sign classifier with Tensorflow\n",
    "\n",
    "## Preparation steps\n",
    "You can download this jupyter notebook with git from here:\n",
    "`git clone https://github.com/olesalscheider/traffic-sign-tutorial`\n",
    "\n",
    "Please follow these preparation steps before the hands-on tutorial so that you come with a prepared system:\n",
    "\n",
    "* Please install all required dependencies:\n",
    "  * Python 3\n",
    "  * Jupyter\n",
    "    * `pip3 install --upgrade jupyter`\n",
    "  * Pillow\n",
    "    * `pip3 install --upgrade pillow`\n",
    "  * urllib3\n",
    "    * `pip3 install --upgrade urllib3`\n",
    "  * numpy\n",
    "    * `pip3 install --upgrade numpy`\n",
    "  * Tensorflow\n",
    "    * `pip3 install --upgrade tensorflow` for the CPU variant\n",
    "    * `pip3 install --upgrade tensorflow-gpu` if you have a GPU with CUDA and CUDNN support\n",
    "    * More details on https://www.tensorflow.org/install/\n",
    "* Download and extract the traffic sign dataset (GTSRB). Execute the first cell with Python code the jupyter notebook to do so.\n",
    "\n",
    "## Prepare the dataset\n",
    "\n",
    "Let's start by splitting the data into a train and a test dataset. We store the filenames in two CSV files and use approximately 80% of the data for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting data...\n",
      "Converting images and splitting datasets...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "DATA_PATH = 'data'\n",
    "DATA_URL = 'http://benchmark.ini.rub.de/Dataset/GTSRB_Final_Training_Images.zip'\n",
    "\n",
    "print('Downloading and extracting data...')\n",
    "with urllib.request.urlopen(DATA_URL) as response:\n",
    "    archive = response.read()\n",
    "    with zipfile.ZipFile(io.BytesIO(archive)) as zip_ref:\n",
    "        zip_ref.extractall(DATA_PATH)\n",
    "\n",
    "print('Converting images and splitting datasets...')\n",
    "train_file_path = os.path.join(DATA_PATH, 'train')\n",
    "test_file_path = os.path.join(DATA_PATH, 'test')\n",
    "with open(train_file_path, 'w') as train_file, open(test_file_path, 'w') as test_file:\n",
    "    # Iterate over all image files in the training data\n",
    "    # directory and store the paths in the CSV files\n",
    "    for dirpath, dirnames, files in os.walk(DATA_PATH):\n",
    "        is_train_example = {}\n",
    "        for file in files:\n",
    "            if file.endswith('.ppm'):\n",
    "                # Convert ppm to png (Tensorflow cannot read ppm)\n",
    "                newfile = file.replace('ppm', 'png')\n",
    "                im = Image.open(os.path.join(dirpath, file))\n",
    "                im.save(os.path.join(dirpath, newfile))\n",
    "                im.close()\n",
    "\n",
    "                # The last directory name encodes the class\n",
    "                # of the training example.\n",
    "                _, label = os.path.split(dirpath)\n",
    "                \n",
    "                # Convert it to an integer (this strips the leading zeros).\n",
    "                label = int(label)\n",
    "\n",
    "                # There are multiple images of each sign.\n",
    "                # The number before the '_' gives the sign number.\n",
    "                # Make sure that different images of the same sign are\n",
    "                # only stored either in the training or the test set.\n",
    "                sign_no = int(file.split('_')[0])\n",
    "\n",
    "                # Generate the string that should be stored in the CSV file.\n",
    "                # It is the image path and the class label.\n",
    "                line = os.path.join(dirpath, newfile) + '\\t' + str(label) + '\\n'\n",
    "\n",
    "                # Store the line either in the training or test CSV file\n",
    "                if not sign_no in is_train_example.keys():\n",
    "                    # keep 80% of the data for training and 20% for testing.\n",
    "                    is_train_example[sign_no] = np.random.randint(0, 10) > 1\n",
    "                if is_train_example[sign_no]:\n",
    "                    train_file.writelines(line)\n",
    "                else:\n",
    "                    test_file.writelines(line)\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a data reader\n",
    "\n",
    "The data reader reads the CSV files for training and testing. These CSV files contain one example per line. This line contains the file name to the image file (png) and the class label (integer number betwenn 0 and 42).\n",
    "\n",
    "The data reader is implemented as a `tf.data.Dataset` for each dataset and a generic `tf.data.Iterator` to iterate over the elements in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "DATA_PATH = 'data'\n",
    "\n",
    "# Create a Tensorflow graph and add all operations to it from now on.\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Define a function that takes a line from the CSV file\n",
    "    # and returns the decoded image and label\n",
    "    def read_data(line):\n",
    "        ##################################\n",
    "        # TODO: Implement this function! #\n",
    "        ##################################\n",
    "        return image, label\n",
    "\n",
    "    ## Create the training dataset\n",
    "    train_dataset = tf.data.TextLineDataset(os.path.join(DATA_PATH, 'train'))\n",
    "    # Shuffle the training dataset\n",
    "    train_dataset = train_dataset.shuffle(30000)\n",
    "    # Call the read_data function for each entry\n",
    "    train_dataset = train_dataset.map(read_data, 2)\n",
    "    # Repeat the dataset 2 times\n",
    "    train_dataset = train_dataset.repeat(2)\n",
    "    # Create batches with 32 training examples\n",
    "    train_dataset = train_dataset.batch(32)\n",
    "\n",
    "    ## Create the test dataset\n",
    "    test_dataset = tf.data.TextLineDataset(os.path.join(DATA_PATH, 'test'))\n",
    "    # Call the read_data function for each entry\n",
    "    test_dataset = test_dataset.map(read_data, 2)\n",
    "    test_dataset = test_dataset.batch(1)\n",
    "\n",
    "    ## Create a generic iterator\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "\n",
    "    ## Create initializer operations for the iterator.\n",
    "    ## These assign either the test of train dataset.\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    test_init_op = iterator.make_initializer(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "First we define a class for one ResNet module. A ResNet module looks like this:\n",
    "\n",
    "![ResNet module](img/resnet_module.png)\n",
    "\n",
    "Variant 1 is used when the number of input channels equals the number of output channels.\n",
    "But if the number of channels is different, the tensors cannot be summed element-wise.\n",
    "In this case we have to use variant 2. Here, we add a 1x1 convolution in the skip connection. This convolution adjusts the number of channels so that both tensors can be summed element-wise.\n",
    "\n",
    "Let's implement this module as a `tf.keras.Model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGULARIZER_WEIGHT = 1e-5\n",
    "\n",
    "class ResnetModule(tf.keras.Model):\n",
    "    def __init__(self, name, num_output_channels):\n",
    "        super().__init__(name=name)\n",
    "        self.num_output_channels = num_output_channels\n",
    "\n",
    "    # The build function is called before using the\n",
    "    # model for the first time. When it is called, the\n",
    "    # input shapes are (partially) known and passed as\n",
    "    # parameter.\n",
    "    # We instantiate the sub-layers in this function.\n",
    "    def build(self, input_shapes):\n",
    "        #########################################\n",
    "        # TODO: Instantiate the sub-layers here #\n",
    "        #########################################\n",
    "\n",
    "    # The call function is called when the model is\n",
    "    # evaluated. We call the sub-layers and simple\n",
    "    # functions to perform the operations of the model.\n",
    "    def call(self, x):\n",
    "        ###############################################\n",
    "        # TODO: Call the sub-layers here to implement #\n",
    "        # the functionality of the module.            #\n",
    "        ###############################################\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a simple model that consists of some ResNet modules. It will look like this:\n",
    "\n",
    "![Network](img/network.png)\n",
    "\n",
    "The first layer is a convolution with a 7x7 kernel, stride 2 and 32 output channels.\n",
    "This is followed by a batch normalization layer.\n",
    "Then three ResNet modules and three 2x2 maxpool layers follow in an alternating fashion. The ResNet Modules have 64, 128 and 256 channels respectively.\n",
    "The last layer is a fully-connected (dense) layer. It reduces the number of channels to the number of traffic sign classes in our dataset (43).\n",
    "\n",
    "Let's also implement this as a `tf.keras.Model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficSignModel(tf.keras.Model):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    # The build function is called before using the\n",
    "    # model for the first time. When it is called, the\n",
    "    # input shapes are (partially) known and passed as\n",
    "    # parameter.\n",
    "    # We instantiate the sub-layers in this function.\n",
    "    def build(self, input_shapes):\n",
    "        self.first_conv = tf.keras.layers.Conv2D(32,\n",
    "            (7, 7),\n",
    "            strides=(2, 2),\n",
    "            activation=tf.keras.activations.relu,\n",
    "            kernel_initializer=tf.keras.initializers.glorot_normal(),\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(REGULARIZER_WEIGHT),\n",
    "            name='first_conv')\n",
    "\n",
    "        ###################################################\n",
    "        # TODO: Instantiate the remaining sub-layers here #\n",
    "        ###################################################\n",
    "\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc = tf.keras.layers.Dense(43) # We have 43 classes\n",
    "\n",
    "    # The call function is called when the model is\n",
    "    # evaluated. We call the sub-layers and simple\n",
    "    # functions to perform the operations of the model.\n",
    "    def call(self, image):\n",
    "        # Cast the image to float\n",
    "        x = tf.cast(image, tf.float32)\n",
    "        # normalize it to a range between -1 and 1\n",
    "        x = (x - tf.constant(128.0, tf.float32)) / tf.constant(128.0, tf.float32)\n",
    "\n",
    "        # Run the neural network layers on the image\n",
    "        x = self.first_conv(x)\n",
    "\n",
    "        ############################################\n",
    "        # TODO: Call the remaining sub-layers here #\n",
    "        ############################################\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the model\n",
    "\n",
    "Now that we have a data reader and defined the model, we can train it.\n",
    "\n",
    "We will instantiate the TrafficSignModel class, pass the data from the dataset iterator as input and get the predictions as output.\n",
    "\n",
    "From this and the ground truth, we can calculate the loss. Here we will use the cross-entropy loss.\n",
    "Then we instantiate the Adam optimizer to minimize the loss function by adjusting the weights of the model.\n",
    "\n",
    "Finally, we will initialize all variables and execute the optimizer in a training loop to get the trained network.\n",
    "We will then evaluate this trained model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the traffic light classifier. This might take a while...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_op' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-380e02fdf293>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Step %4i - Accuracy: %.4f, loss: %.4f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_op' is not defined"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    session = tf.Session()\n",
    "\n",
    "    ###############################################################\n",
    "    # TODO: Instantiate the model and implement operations        #\n",
    "    # to train ('train_op'), to get the accuracy ('accuracy_op'), #\n",
    "    # and to get the loss ('loss_op')                             #\n",
    "    ###############################################################\n",
    "\n",
    "    # Initialize the model variables (randomly).\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print('Train the traffic light classifier. This might take a while...')\n",
    "    # Initialize the dataset iterator for training.\n",
    "    session.run(train_init_op)\n",
    "    i = 0\n",
    "    while True:\n",
    "        try:\n",
    "            _, accuracy, loss = session.run([train_op, accuracy_op, loss_op])\n",
    "            if i % 20 == 0:\n",
    "                print('Step %4i - Accuracy: %.4f, loss: %.4f' % (i, accuracy, loss))\n",
    "            i += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break # We finished!\n",
    "    \n",
    "    print('Evaluate the classifier. This might take a while...')\n",
    "    # Initialize the dataset iterator for training.\n",
    "    session.run(test_init_op)\n",
    "    i = 0\n",
    "    total_accuracy = 0.0\n",
    "    while True:\n",
    "        try:\n",
    "            accuracy = session.run(accuracy_op)\n",
    "            total_accuracy += accuracy\n",
    "            i += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break # We finished!\n",
    "    print('Mean accuracy on the test dataset: %.4f' % (total_accuracy / i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
